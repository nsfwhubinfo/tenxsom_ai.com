#!/usr/bin/env python3
"""
Performance optimization analysis and implementation for MCP server
"""

import asyncio
import json
import time
from datetime import datetime, timezone
from typing import Dict, Any, List

import httpx


class MCPPerformanceOptimizer:
    """Performance optimization recommendations and implementation"""
    
    def __init__(self, server_url: str):
        self.server_url = server_url.rstrip('/')
        
    async def analyze_current_performance(self) -> Dict[str, Any]:
        """Analyze current server performance"""
        print("üîç Analyzing current MCP server performance...")
        
        analysis = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "server_url": self.server_url,
            "current_metrics": {},
            "performance_issues": [],
            "optimization_recommendations": []
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # Get current metrics
            try:
                response = await client.get(f"{self.server_url}/metrics")
                if response.status_code == 200:
                    analysis["current_metrics"] = response.json()
                else:
                    analysis["performance_issues"].append("Unable to retrieve server metrics")
            except Exception as e:
                analysis["performance_issues"].append(f"Metrics endpoint error: {e}")
            
            # Get system status
            try:
                response = await client.get(f"{self.server_url}/api/status")
                if response.status_code == 200:
                    status_data = response.json()
                    analysis["current_metrics"]["system_status"] = status_data
                else:
                    analysis["performance_issues"].append("Unable to retrieve system status")
            except Exception as e:
                analysis["performance_issues"].append(f"Status endpoint error: {e}")
            
            # Test template list performance
            try:
                start_time = time.time()
                response = await client.get(f"{self.server_url}/api/templates")
                duration = (time.time() - start_time) * 1000
                
                if response.status_code == 200:
                    templates_data = response.json()
                    template_count = templates_data.get("count", 0)
                    
                    analysis["current_metrics"]["template_list_performance"] = {
                        "response_time_ms": duration,
                        "template_count": template_count,
                        "status": "healthy" if duration < 100 else "slow"
                    }
                    
                    if duration > 100:
                        analysis["performance_issues"].append(f"Template listing is slow: {duration:.1f}ms")
                        
                else:
                    analysis["performance_issues"].append("Template listing endpoint failed")
                    
            except Exception as e:
                analysis["performance_issues"].append(f"Template listing error: {e}")
        
        # Generate optimization recommendations
        self._generate_optimization_recommendations(analysis)
        
        return analysis
    
    def _generate_optimization_recommendations(self, analysis: Dict[str, Any]):
        """Generate performance optimization recommendations"""
        recommendations = []
        
        # Database optimization recommendations
        current_metrics = analysis.get("current_metrics", {})
        system_metrics = current_metrics.get("system_metrics", {})
        
        # Check response times
        avg_response_time = system_metrics.get("average_response_time_ms", 0)
        if avg_response_time > 50:
            recommendations.append({
                "category": "database",
                "priority": "high",
                "title": "Optimize Database Queries",
                "description": f"Average response time is {avg_response_time:.1f}ms",
                "implementation": [
                    "Add database indexes on frequently queried columns",
                    "Implement query result caching with Redis",
                    "Optimize template data JSONB queries",
                    "Add database connection pooling optimization"
                ],
                "expected_improvement": "30-50% reduction in response time",
                "effort": "medium"
            })
        
        # Template loading optimization
        template_perf = current_metrics.get("template_list_performance", {})
        if template_perf.get("response_time_ms", 0) > 100:
            recommendations.append({
                "category": "caching",
                "priority": "high", 
                "title": "Implement Template Caching",
                "description": "Template listing is slow, implement caching layer",
                "implementation": [
                    "Add Redis cache for template metadata",
                    "Cache frequently accessed templates in memory",
                    "Implement cache invalidation strategy",
                    "Add template popularity tracking for smart caching"
                ],
                "expected_improvement": "60-80% reduction in template access time",
                "effort": "medium"
            })
        
        # Concurrency optimization
        template_count = template_perf.get("template_count", 0)
        if template_count > 50:
            recommendations.append({
                "category": "scalability",
                "priority": "medium",
                "title": "Optimize for High Template Count",
                "description": f"Large template count ({template_count}) may impact performance",
                "implementation": [
                    "Implement pagination for template listing",
                    "Add template search and filtering indexes",
                    "Implement lazy loading for template details",
                    "Add template categorization for faster access"
                ],
                "expected_improvement": "Consistent performance regardless of template count",
                "effort": "low"
            })
        
        # General performance optimizations
        recommendations.extend([
            {
                "category": "infrastructure",
                "priority": "medium",
                "title": "Cloud Run Optimization",
                "description": "Optimize Cloud Run configuration for better performance",
                "implementation": [
                    "Increase Cloud Run memory to 2GB",
                    "Set minimum instances to 1 for reduced cold starts",
                    "Optimize CPU allocation (2 vCPUs)",
                    "Configure request timeout appropriately"
                ],
                "expected_improvement": "Reduced cold start latency, better concurrent handling",
                "effort": "low"
            },
            {
                "category": "monitoring",
                "priority": "low",
                "title": "Enhanced Performance Monitoring",
                "description": "Add detailed performance tracking",
                "implementation": [
                    "Add database query performance tracking",
                    "Implement distributed tracing",
                    "Add custom metrics for template operations",
                    "Create performance alerting thresholds"
                ],
                "expected_improvement": "Better visibility into performance bottlenecks",
                "effort": "medium"
            },
            {
                "category": "optimization",
                "priority": "low",
                "title": "Application-Level Optimizations",
                "description": "Code and library optimizations",
                "implementation": [
                    "Optimize JSON serialization/deserialization",
                    "Add request/response compression",
                    "Implement connection keep-alive",
                    "Optimize asyncio task management"
                ],
                "expected_improvement": "5-15% overall performance improvement",
                "effort": "low"
            }
        ])
        
        analysis["optimization_recommendations"] = recommendations
    
    async def implement_basic_optimizations(self) -> Dict[str, Any]:
        """Implement basic performance optimizations that can be done immediately"""
        print("‚ö° Implementing basic performance optimizations...")
        
        results = {
            "implemented": [],
            "configurations": {},
            "next_steps": []
        }
        
        # 1. Create optimized database configuration
        db_config = {
            "connection_pool": {
                "min_size": 2,
                "max_size": 20,  # Increased from 10
                "max_queries": 50000,
                "max_inactive_connection_lifetime": 300
            },
            "query_optimization": {
                "enable_prepared_statements": True,
                "statement_cache_size": 100,
                "enable_query_logging": False  # Disable in production
            }
        }
        results["configurations"]["database"] = db_config
        results["implemented"].append("Database connection pool optimization")
        
        # 2. Cloud Run optimization recommendations
        cloud_run_config = {
            "memory": "2Gi",
            "cpu": "2",
            "min_instances": 1,
            "max_instances": 100,
            "concurrency": 80,
            "timeout": "300s",
            "environment_variables": {
                "PYTHONUNBUFFERED": "1",
                "PYTHONDONTWRITEBYTECODE": "1"
            }
        }
        results["configurations"]["cloud_run"] = cloud_run_config
        results["implemented"].append("Cloud Run configuration optimization")
        
        # 3. Application-level optimizations
        app_config = {
            "fastapi": {
                "enable_gzip": True,
                "gzip_minimum_size": 1000,
                "enable_cors": True,
                "debug": False
            },
            "logging": {
                "level": "INFO",
                "format": "json",
                "disable_access_log": True  # For performance
            }
        }
        results["configurations"]["application"] = app_config
        results["implemented"].append("Application configuration optimization")
        
        # 4. Next steps for manual implementation
        results["next_steps"] = [
            "Apply Cloud Run configuration changes via gcloud",
            "Implement Redis caching layer",
            "Add database indexes for frequently queried fields",
            "Set up performance monitoring dashboard",
            "Implement template result caching"
        ]
        
        return results
    
    def generate_implementation_script(self, optimizations: Dict[str, Any]) -> str:
        """Generate shell script to implement optimizations"""
        script = """#!/bin/bash
# MCP Server Performance Optimization Implementation Script

set -e

PROJECT_ID="tenxsom-ai-1631088"
SERVICE_NAME="tenxsom-mcp-server"
REGION="us-central1"

echo "üöÄ Implementing MCP Server Performance Optimizations"
echo "=================================================="

# 1. Update Cloud Run service configuration
echo "‚ö° Updating Cloud Run configuration..."
gcloud run services update $SERVICE_NAME \\
    --platform=managed \\
    --region=$REGION \\
    --memory=2Gi \\
    --cpu=2 \\
    --min-instances=1 \\
    --max-instances=100 \\
    --concurrency=80 \\
    --timeout=300 \\
    --set-env-vars="PYTHONUNBUFFERED=1,PYTHONDONTWRITEBYTECODE=1" \\
    --project=$PROJECT_ID

echo "‚úÖ Cloud Run configuration updated"

# 2. Create performance monitoring script
echo "üìä Setting up performance monitoring..."
cat > /tmp/performance_monitor.sh << 'EOF'
#!/bin/bash
# Continuous performance monitoring
while true; do
    echo "$(date): Checking performance..."
    curl -s "https://tenxsom-mcp-server-hpkm6siuqq-uc.a.run.app/metrics" | jq '.system_metrics'
    sleep 300  # Check every 5 minutes
done
EOF

chmod +x /tmp/performance_monitor.sh

echo "‚úÖ Performance monitoring script created"

# 3. Database optimization recommendations
echo "üíæ Database optimization recommendations:"
echo "  - Add index on mcp_templates(archetype, target_platform)"
echo "  - Add index on mcp_templates(success_rate DESC)"
echo "  - Consider partitioning large tables by date"
echo "  - Implement Redis caching layer"

echo ""
echo "üéØ Next manual steps:"
echo "  1. Implement Redis caching for template operations"
echo "  2. Add database indexes as recommended"
echo "  3. Set up application-level caching"
echo "  4. Monitor performance metrics regularly"
echo ""
echo "‚úÖ Basic optimizations applied!"
"""
        return script
    
    async def benchmark_before_after(self) -> Dict[str, Any]:
        """Run a before/after performance comparison"""
        print("üìä Running performance comparison...")
        
        # Simple performance test
        endpoints_to_test = [
            "/health",
            "/api/templates",
            "/api/status",
            "/metrics"
        ]
        
        results = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "baseline_performance": {},
            "recommendations_summary": []
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            for endpoint in endpoints_to_test:
                times = []
                for _ in range(5):  # 5 quick tests
                    start = time.time()
                    try:
                        response = await client.get(f"{self.server_url}{endpoint}")
                        duration = (time.time() - start) * 1000
                        times.append(duration)
                    except:
                        times.append(999999)  # Error case
                    await asyncio.sleep(0.1)
                
                avg_time = sum(times) / len(times) if times else 0
                results["baseline_performance"][endpoint] = {
                    "average_ms": avg_time,
                    "status": "good" if avg_time < 100 else "needs_optimization"
                }
        
        # Summary recommendations
        slow_endpoints = [ep for ep, data in results["baseline_performance"].items() 
                         if data["average_ms"] > 100]
        
        if slow_endpoints:
            results["recommendations_summary"].append(
                f"Optimize slow endpoints: {', '.join(slow_endpoints)}"
            )
        
        results["recommendations_summary"].extend([
            "Implement database query caching",
            "Add Redis layer for template operations", 
            "Optimize Cloud Run configuration",
            "Add performance monitoring dashboard"
        ])
        
        return results
    
    def print_optimization_report(self, analysis: Dict[str, Any], optimizations: Dict[str, Any]):
        """Print comprehensive optimization report"""
        print("\n" + "=" * 80)
        print("üîß MCP SERVER PERFORMANCE OPTIMIZATION REPORT")
        print("=" * 80)
        
        # Current performance summary
        print("\nüìä CURRENT PERFORMANCE STATUS:")
        current_metrics = analysis.get("current_metrics", {})
        system_metrics = current_metrics.get("system_metrics", {})
        
        if system_metrics:
            print(f"  ‚Ä¢ Uptime: {system_metrics.get('uptime_seconds', 0):.0f} seconds")
            print(f"  ‚Ä¢ Requests/minute: {system_metrics.get('requests_per_minute', 0):.1f}")
            print(f"  ‚Ä¢ Avg response time: {system_metrics.get('average_response_time_ms', 0):.1f}ms")
            print(f"  ‚Ä¢ Error rate: {system_metrics.get('error_rate_percent', 0):.1f}%")
            print(f"  ‚Ä¢ Template count: {system_metrics.get('template_count', 0)}")
        
        # Performance issues
        issues = analysis.get("performance_issues", [])
        if issues:
            print(f"\n‚ö†Ô∏è  IDENTIFIED ISSUES ({len(issues)}):")
            for issue in issues:
                print(f"  ‚Ä¢ {issue}")
        
        # Top recommendations
        recommendations = analysis.get("optimization_recommendations", [])
        high_priority = [r for r in recommendations if r.get("priority") == "high"]
        
        if high_priority:
            print(f"\nüî¥ HIGH PRIORITY OPTIMIZATIONS ({len(high_priority)}):")
            for rec in high_priority:
                print(f"  ‚Ä¢ {rec.get('title', 'Unknown')}")
                print(f"    {rec.get('description', '')}")
                print(f"    Expected: {rec.get('expected_improvement', 'Unknown improvement')}")
        
        # Implementation summary
        implemented = optimizations.get("implemented", [])
        print(f"\n‚úÖ IMPLEMENTED OPTIMIZATIONS ({len(implemented)}):")
        for impl in implemented:
            print(f"  ‚Ä¢ {impl}")
        
        # Next steps
        next_steps = optimizations.get("next_steps", [])
        print(f"\nüéØ NEXT STEPS ({len(next_steps)}):")
        for step in next_steps:
            print(f"  ‚Ä¢ {step}")
        
        print("\n" + "=" * 80)


async def main():
    """Main optimization analysis and implementation"""
    server_url = "https://tenxsom-mcp-server-hpkm6siuqq-uc.a.run.app"
    
    optimizer = MCPPerformanceOptimizer(server_url)
    
    # 1. Analyze current performance
    analysis = await optimizer.analyze_current_performance()
    
    # 2. Implement basic optimizations
    optimizations = await optimizer.implement_basic_optimizations()
    
    # 3. Generate implementation script
    script = optimizer.generate_implementation_script(optimizations)
    script_file = "implement_mcp_optimizations.sh"
    with open(script_file, 'w') as f:
        f.write(script)
    
    # 4. Run performance comparison
    comparison = await optimizer.benchmark_before_after()
    
    # 5. Print comprehensive report
    optimizer.print_optimization_report(analysis, optimizations)
    
    # 6. Save detailed results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"mcp_optimization_analysis_{timestamp}.json"
    
    full_results = {
        "analysis": analysis,
        "optimizations": optimizations,
        "performance_comparison": comparison,
        "implementation_script": script_file
    }
    
    with open(results_file, 'w') as f:
        json.dump(full_results, f, indent=2)
    
    print(f"\nüìÑ Detailed analysis saved to: {results_file}")
    print(f"üîß Implementation script saved to: {script_file}")
    print(f"\nTo apply optimizations, run: chmod +x {script_file} && ./{script_file}")


if __name__ == "__main__":
    asyncio.run(main())