#!/usr/bin/env python3

"""
Predictive Analytics Engine for TenxsomAI
Real-time optimization using machine learning and predictive feedback loops
"""

import asyncio
import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import pickle
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, accuracy_score

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PredictionMetrics:
    """Prediction accuracy metrics"""
    model_name: str
    mae: float  # Mean Absolute Error
    accuracy: float
    confidence_interval: Tuple[float, float]
    last_training: datetime
    prediction_count: int


@dataclass
class ContentPerformancePrediction:
    """Predicted performance for content"""
    content_id: str
    predicted_views: int
    predicted_engagement_rate: float
    predicted_ctr: float
    predicted_revenue: float
    viral_probability: float
    optimization_recommendations: List[str]
    confidence_score: float


@dataclass
class TrendPrediction:
    """Predicted trend evolution"""
    keyword: str
    current_momentum: float
    predicted_peak_time: datetime
    predicted_peak_value: float
    decay_rate: float
    opportunity_window: Tuple[datetime, datetime]
    confidence: float


class PredictiveAnalyticsEngine:
    """
    Advanced predictive analytics system for content optimization
    
    Features:
    - Performance prediction using ML models
    - Trend evolution forecasting
    - Real-time optimization recommendations
    - Viral content detection
    - Revenue optimization predictions
    - A/B testing outcome prediction
    """
    
    def __init__(self, config_manager=None):
        """Initialize predictive analytics engine"""
        self.config = config_manager
        
        # Model storage
        self.models = {
            "views_predictor": None,
            "engagement_predictor": None,
            "revenue_predictor": None,
            "viral_detector": None,
            "trend_forecaster": None
        }
        
        # Feature scalers
        self.scalers = {}
        
        # Model metrics
        self.model_metrics = {}
        
        # Prediction cache
        self.prediction_cache = {}
        self.cache_ttl = 3600  # 1 hour
        
        # Training data storage
        self.training_data_path = Path("analytics_training_data")
        self.training_data_path.mkdir(exist_ok=True)
        
        # Performance tracking
        self.performance_history = []
        self.prediction_accuracy_history = []
        
        # Initialize models
        self._initialize_models()
        self._load_existing_models()
        
    def _initialize_models(self):
        """Initialize ML models with default configurations"""
        
        # Views prediction model (Random Forest)
        self.models["views_predictor"] = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        # Engagement prediction model (Random Forest)
        self.models["engagement_predictor"] = RandomForestRegressor(
            n_estimators=80,
            max_depth=8,
            random_state=42,
            n_jobs=-1
        )
        
        # Revenue prediction model (Gradient Boosting)
        self.models["revenue_predictor"] = RandomForestRegressor(
            n_estimators=120,
            max_depth=12,
            random_state=42,
            n_jobs=-1
        )
        
        # Viral content detector (Gradient Boosting Classifier)
        self.models["viral_detector"] = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        )
        
        # Trend forecaster (Random Forest)
        self.models["trend_forecaster"] = RandomForestRegressor(
            n_estimators=150,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        
        # Initialize scalers
        for model_name in self.models.keys():
            self.scalers[model_name] = StandardScaler()
            
    def _load_existing_models(self):
        """Load pre-trained models if available"""
        
        models_path = Path("saved_models")
        models_path.mkdir(exist_ok=True)
        
        for model_name in self.models.keys():
            model_file = models_path / f"{model_name}.pkl"
            scaler_file = models_path / f"{model_name}_scaler.pkl"
            
            try:
                if model_file.exists() and scaler_file.exists():
                    with open(model_file, 'rb') as f:
                        self.models[model_name] = pickle.load(f)
                    
                    with open(scaler_file, 'rb') as f:
                        self.scalers[model_name] = pickle.load(f)
                    
                    logger.info(f"ðŸ“ˆ Loaded pre-trained model: {model_name}")
                    
            except Exception as e:
                logger.warning(f"Failed to load model {model_name}: {e}")
                
    def _save_models(self):
        """Save trained models to disk"""
        
        models_path = Path("saved_models")
        models_path.mkdir(exist_ok=True)
        
        for model_name, model in self.models.items():
            if model is not None:
                try:
                    model_file = models_path / f"{model_name}.pkl"
                    scaler_file = models_path / f"{model_name}_scaler.pkl"
                    
                    with open(model_file, 'wb') as f:
                        pickle.dump(model, f)
                    
                    with open(scaler_file, 'wb') as f:
                        pickle.dump(self.scalers[model_name], f)
                        
                except Exception as e:
                    logger.error(f"Failed to save model {model_name}: {e}")
                    
    async def predict_content_performance(self, 
                                        content_metadata: Dict[str, Any],
                                        historical_context: Dict[str, Any] = None) -> ContentPerformancePrediction:
        """
        Predict comprehensive content performance metrics
        
        Args:
            content_metadata: Content information (title, description, category, etc.)
            historical_context: Historical performance data for context
            
        Returns:
            Comprehensive performance prediction
        """
        
        content_id = content_metadata.get("content_id", f"pred_{int(time.time())}")
        
        # Check cache first
        cache_key = f"content_perf_{hash(json.dumps(content_metadata, sort_keys=True))}"
        if cache_key in self.prediction_cache:
            cached_result, timestamp = self.prediction_cache[cache_key]
            if (datetime.now() - timestamp).seconds < self.cache_ttl:
                return cached_result
        
        # Extract features for prediction
        features = self._extract_content_features(content_metadata, historical_context)
        
        # Predict individual metrics
        predicted_views = await self._predict_views(features)
        predicted_engagement = await self._predict_engagement(features)
        predicted_revenue = await self._predict_revenue(features)
        viral_probability = await self._predict_viral_probability(features)
        
        # Calculate CTR based on views and engagement
        predicted_ctr = predicted_engagement * 0.15  # Typical correlation
        
        # Generate optimization recommendations
        optimization_recommendations = self._generate_optimization_recommendations(
            features, predicted_views, predicted_engagement, viral_probability
        )
        
        # Calculate overall confidence score
        confidence_score = self._calculate_prediction_confidence(features)
        
        # Create prediction object
        prediction = ContentPerformancePrediction(
            content_id=content_id,
            predicted_views=int(predicted_views),
            predicted_engagement_rate=round(predicted_engagement, 4),
            predicted_ctr=round(predicted_ctr, 4),
            predicted_revenue=round(predicted_revenue, 2),
            viral_probability=round(viral_probability, 3),
            optimization_recommendations=optimization_recommendations,
            confidence_score=round(confidence_score, 3)
        )
        
        # Cache result
        self.prediction_cache[cache_key] = (prediction, datetime.now())
        
        logger.info(f"ðŸ“Š Predicted performance for {content_id}: {predicted_views} views, {viral_probability:.3f} viral probability")
        
        return prediction
    
    async def predict_trend_evolution(self, 
                                    keyword: str,
                                    current_metrics: Dict[str, Any],
                                    time_horizon_hours: int = 48) -> TrendPrediction:
        """
        Predict how a trend will evolve over time
        
        Args:
            keyword: Trend keyword
            current_metrics: Current trend metrics
            time_horizon_hours: Prediction time horizon
            
        Returns:
            Trend evolution prediction
        """
        
        # Extract trend features
        trend_features = self._extract_trend_features(keyword, current_metrics)
        
        # Predict trend evolution
        momentum_prediction = await self._predict_trend_momentum(trend_features)
        
        # Calculate peak time and value
        current_time = datetime.now()
        
        # Model trend as exponential growth/decay
        current_value = current_metrics.get("search_volume", 1000)
        growth_rate = current_metrics.get("growth_rate", 0.1)
        
        # Predict peak (usually 6-24 hours for viral trends)
        peak_hours = min(6 + momentum_prediction * 18, 24)
        predicted_peak_time = current_time + timedelta(hours=peak_hours)
        
        # Peak value calculation
        peak_multiplier = 1.5 + momentum_prediction * 3  # 1.5x to 4.5x current value
        predicted_peak_value = current_value * peak_multiplier
        
        # Decay rate (higher momentum = slower decay)
        decay_rate = 0.8 - momentum_prediction * 0.3  # 0.5 to 0.8 daily decay
        
        # Opportunity window (before peak + some time after)
        window_start = current_time
        window_end = predicted_peak_time + timedelta(hours=6)
        
        # Confidence based on data quality and model certainty
        confidence = min(0.95, 0.6 + momentum_prediction * 0.3)
        
        prediction = TrendPrediction(
            keyword=keyword,
            current_momentum=round(momentum_prediction, 3),
            predicted_peak_time=predicted_peak_time,
            predicted_peak_value=round(predicted_peak_value),
            decay_rate=round(decay_rate, 3),
            opportunity_window=(window_start, window_end),
            confidence=round(confidence, 3)
        )
        
        logger.info(f"ðŸ“ˆ Trend prediction for '{keyword}': Peak at {predicted_peak_time.strftime('%H:%M')}, {predicted_peak_value:.0f} volume")
        
        return prediction
    
    async def optimize_posting_schedule(self, 
                                      content_queue: List[Dict[str, Any]],
                                      optimization_window_hours: int = 24) -> Dict[str, Any]:
        """
        Optimize posting schedule for maximum performance
        
        Args:
            content_queue: Queue of content to be posted
            optimization_window_hours: Time window for optimization
            
        Returns:
            Optimized posting schedule
        """
        
        optimized_schedule = []
        current_time = datetime.now()
        
        # Analyze each piece of content
        for content in content_queue:
            # Predict performance at different posting times
            best_time = None
            best_score = 0
            
            # Test posting times at 2-hour intervals
            for hour_offset in range(0, optimization_window_hours, 2):
                posting_time = current_time + timedelta(hours=hour_offset)
                
                # Add time-based features
                time_features = self._extract_time_features(posting_time)
                content_with_time = {**content, **time_features}
                
                # Predict performance at this time
                performance_prediction = await self.predict_content_performance(content_with_time)
                
                # Calculate combined score (views + engagement + viral potential)
                score = (
                    performance_prediction.predicted_views * 0.4 +
                    performance_prediction.predicted_engagement_rate * 10000 * 0.3 +
                    performance_prediction.viral_probability * 50000 * 0.3
                )
                
                if score > best_score:
                    best_score = score
                    best_time = posting_time
            
            optimized_schedule.append({
                "content": content,
                "optimal_posting_time": best_time,
                "predicted_score": best_score,
                "optimization_confidence": 0.75  # Base confidence
            })
        
        # Sort by predicted score (highest first)
        optimized_schedule.sort(key=lambda x: x["predicted_score"], reverse=True)
        
        # Avoid clustering posts too closely
        optimized_schedule = self._distribute_posting_times(optimized_schedule)
        
        return {
            "optimized_schedule": optimized_schedule,
            "total_content": len(optimized_schedule),
            "optimization_window": optimization_window_hours,
            "expected_performance_improvement": self._calculate_schedule_improvement(optimized_schedule)
        }
    
    async def predict_ab_test_outcomes(self, 
                                     test_variants: List[Dict[str, Any]],
                                     test_duration_hours: int = 24) -> Dict[str, Any]:
        """
        Predict A/B test outcomes before running the test
        
        Args:
            test_variants: List of test variants
            test_duration_hours: Expected test duration
            
        Returns:
            Predicted test outcomes
        """
        
        variant_predictions = []
        
        for i, variant in enumerate(test_variants):
            # Predict performance for each variant
            performance_pred = await self.predict_content_performance(variant)
            
            # Estimate statistical significance timeline
            expected_views = performance_pred.predicted_views
            expected_conversion_rate = performance_pred.predicted_engagement_rate
            
            # Statistical power calculation (simplified)
            min_detectable_effect = 0.05  # 5% improvement
            significance_hours = self._estimate_significance_time(
                expected_views, expected_conversion_rate, min_detectable_effect
            )
            
            variant_predictions.append({
                "variant_id": i,
                "variant_name": variant.get("name", f"Variant_{i}"),
                "predicted_performance": asdict(performance_pred),
                "estimated_significance_hours": significance_hours,
                "recommended_traffic_split": self._calculate_optimal_traffic_split(performance_pred),
                "risk_assessment": self._assess_variant_risk(performance_pred)
            })
        
        # Determine predicted winner
        best_variant = max(variant_predictions, key=lambda x: x["predicted_performance"]["predicted_views"])
        
        return {
            "variant_predictions": variant_predictions,
            "predicted_winner": best_variant,
            "recommended_test_duration": max(v["estimated_significance_hours"] for v in variant_predictions),
            "confidence_in_prediction": self._calculate_ab_test_confidence(variant_predictions),
            "early_stopping_recommendation": self._recommend_early_stopping(variant_predictions)
        }
    
    async def train_models_with_new_data(self, 
                                       performance_data: List[Dict[str, Any]],
                                       retrain_threshold: int = 100):
        """
        Train models with new performance data
        
        Args:
            performance_data: New performance data points
            retrain_threshold: Minimum data points before retraining
        """
        
        if len(performance_data) < retrain_threshold:
            logger.info(f"Insufficient data for retraining ({len(performance_data)} < {retrain_threshold})")
            return
        
        logger.info(f"ðŸŽ“ Training models with {len(performance_data)} new data points")
        
        # Convert to training format
        training_features, training_labels = self._prepare_training_data(performance_data)
        
        # Train each model
        for model_name in self.models.keys():
            try:
                await self._train_individual_model(model_name, training_features, training_labels)
                
            except Exception as e:
                logger.error(f"Failed to train {model_name}: {e}")
        
        # Save updated models
        self._save_models()
        
        # Update model metrics
        await self._evaluate_model_performance(training_features, training_labels)
        
        logger.info("âœ… Model training completed")
    
    async def _predict_views(self, features: np.ndarray) -> float:
        """Predict view count"""
        
        if self.models["views_predictor"] is None:
            return self._fallback_views_prediction(features)
        
        try:
            scaled_features = self.scalers["views_predictor"].transform([features])
            prediction = self.models["views_predictor"].predict(scaled_features)[0]
            return max(100, prediction)  # Minimum 100 views
            
        except Exception as e:
            logger.warning(f"Views prediction failed: {e}")
            return self._fallback_views_prediction(features)
    
    async def _predict_engagement(self, features: np.ndarray) -> float:
        """Predict engagement rate"""
        
        if self.models["engagement_predictor"] is None:
            return self._fallback_engagement_prediction(features)
        
        try:
            scaled_features = self.scalers["engagement_predictor"].transform([features])
            prediction = self.models["engagement_predictor"].predict(scaled_features)[0]
            return max(0.01, min(0.20, prediction))  # 1% to 20% engagement
            
        except Exception as e:
            logger.warning(f"Engagement prediction failed: {e}")
            return self._fallback_engagement_prediction(features)
    
    async def _predict_revenue(self, features: np.ndarray) -> float:
        """Predict revenue"""
        
        if self.models["revenue_predictor"] is None:
            return self._fallback_revenue_prediction(features)
        
        try:
            scaled_features = self.scalers["revenue_predictor"].transform([features])
            prediction = self.models["revenue_predictor"].predict(scaled_features)[0]
            return max(0.10, prediction)  # Minimum $0.10
            
        except Exception as e:
            logger.warning(f"Revenue prediction failed: {e}")
            return self._fallback_revenue_prediction(features)
    
    async def _predict_viral_probability(self, features: np.ndarray) -> float:
        """Predict viral probability"""
        
        if self.models["viral_detector"] is None:
            return self._fallback_viral_prediction(features)
        
        try:
            scaled_features = self.scalers["viral_detector"].transform([features])
            prediction = self.models["viral_detector"].predict_proba(scaled_features)[0]
            return prediction[1] if len(prediction) > 1 else 0.1  # Probability of viral class
            
        except Exception as e:
            logger.warning(f"Viral prediction failed: {e}")
            return self._fallback_viral_prediction(features)
    
    async def _predict_trend_momentum(self, features: np.ndarray) -> float:
        """Predict trend momentum"""
        
        if self.models["trend_forecaster"] is None:
            return 0.5  # Neutral momentum
        
        try:
            scaled_features = self.scalers["trend_forecaster"].transform([features])
            prediction = self.models["trend_forecaster"].predict(scaled_features)[0]
            return max(0.0, min(1.0, prediction))  # 0 to 1 momentum
            
        except Exception as e:
            logger.warning(f"Trend momentum prediction failed: {e}")
            return 0.5
    
    def _extract_content_features(self, 
                                content_metadata: Dict[str, Any],
                                historical_context: Dict[str, Any] = None) -> np.ndarray:
        """Extract features from content metadata for ML models"""
        
        features = []
        
        # Basic content features
        title = content_metadata.get("title", "")
        description = content_metadata.get("description", "")
        category = content_metadata.get("category", "general")
        duration = content_metadata.get("duration", 30)
        quality_tier = content_metadata.get("quality_tier", "volume")
        
        # Text analysis features
        features.extend([
            len(title),  # Title length
            len(description),  # Description length
            title.count(" ") + 1,  # Word count in title
            len([c for c in title if c.isupper()]),  # Uppercase characters
            title.count("!") + title.count("?"),  # Exclamation/question marks
        ])
        
        # Category encoding (one-hot style)
        categories = ["business", "tech", "education", "health", "lifestyle", "entertainment", "gaming", "music", "sports", "news"]
        category_features = [1 if category.lower() == cat else 0 for cat in categories]
        features.extend(category_features)
        
        # Quality tier encoding
        tier_features = [
            1 if quality_tier == "premium" else 0,
            1 if quality_tier == "standard" else 0,
            1 if quality_tier == "volume" else 0
        ]
        features.extend(tier_features)
        
        # Duration and technical features
        features.extend([
            duration,
            content_metadata.get("estimated_views", 1000),
            content_metadata.get("trending_score", 0.5),
            content_metadata.get("competition_level", 0.5),
        ])
        
        # Time-based features
        now = datetime.now()
        features.extend([
            now.hour,  # Hour of day
            now.weekday(),  # Day of week
            now.day,  # Day of month
            1 if now.weekday() >= 5 else 0,  # Weekend flag
        ])
        
        # Historical context features
        if historical_context:
            features.extend([
                historical_context.get("avg_views_last_week", 2000),
                historical_context.get("avg_engagement_last_week", 0.05),
                historical_context.get("trending_topics_count", 5),
            ])
        else:
            features.extend([2000, 0.05, 5])  # Default values
        
        return np.array(features, dtype=float)
    
    def _extract_trend_features(self, keyword: str, current_metrics: Dict[str, Any]) -> np.ndarray:
        """Extract features for trend prediction"""
        
        features = []
        
        # Keyword features
        features.extend([
            len(keyword),
            keyword.count(" ") + 1,  # Word count
            len([c for c in keyword if c.isupper()]),  # Uppercase count
        ])
        
        # Current metrics
        features.extend([
            current_metrics.get("search_volume", 1000),
            current_metrics.get("growth_rate", 0.1),
            current_metrics.get("competition_level", 0.5),
            current_metrics.get("social_mentions", 100),
            current_metrics.get("news_mentions", 10),
        ])
        
        # Time features
        now = datetime.now()
        features.extend([
            now.hour,
            now.weekday(),
            1 if now.weekday() >= 5 else 0,
        ])
        
        return np.array(features, dtype=float)
    
    def _extract_time_features(self, posting_time: datetime) -> Dict[str, Any]:
        """Extract time-based features for optimal posting"""
        
        return {
            "posting_hour": posting_time.hour,
            "posting_day": posting_time.weekday(),
            "is_weekend": 1 if posting_time.weekday() >= 5 else 0,
            "is_prime_time": 1 if 18 <= posting_time.hour <= 22 else 0,  # 6-10 PM
            "posting_timestamp": posting_time.timestamp()
        }
    
    def _generate_optimization_recommendations(self, 
                                             features: np.ndarray,
                                             predicted_views: float,
                                             predicted_engagement: float,
                                             viral_probability: float) -> List[str]:
        """Generate optimization recommendations based on predictions"""
        
        recommendations = []
        
        # Title optimization
        title_length = features[0] if len(features) > 0 else 50
        if title_length < 30:
            recommendations.append("Extend title for better SEO (target 30-60 characters)")
        elif title_length > 80:
            recommendations.append("Shorten title for better readability (target 30-60 characters)")
        
        # Engagement optimization
        if predicted_engagement < 0.04:
            recommendations.append("Add call-to-action to improve engagement")
            recommendations.append("Consider more interactive content elements")
        
        # Viral potential optimization
        if viral_probability > 0.3:
            recommendations.append("High viral potential - prioritize immediate posting")
            recommendations.append("Prepare for increased engagement and community management")
        elif viral_probability < 0.1:
            recommendations.append("Low viral potential - focus on SEO optimization")
        
        # Views optimization
        if predicted_views < 1000:
            recommendations.append("Improve thumbnail and title for better CTR")
            recommendations.append("Consider trending keywords in description")
        
        return recommendations
    
    def _calculate_prediction_confidence(self, features: np.ndarray) -> float:
        """Calculate confidence in predictions based on feature quality"""
        
        # Simple confidence calculation based on feature completeness
        non_zero_features = np.count_nonzero(features)
        total_features = len(features)
        
        feature_completeness = non_zero_features / total_features
        
        # Base confidence with feature completeness factor
        confidence = 0.6 + 0.3 * feature_completeness
        
        return min(0.95, confidence)
    
    def _fallback_views_prediction(self, features: np.ndarray) -> float:
        """Fallback views prediction when model is unavailable"""
        
        # Simple heuristic based on basic features
        base_views = 1500
        
        if len(features) > 10:
            # Adjust based on category and quality
            category_boost = sum(features[5:15]) * 300  # Category features
            quality_boost = sum(features[15:18]) * 500  # Quality features
            base_views += category_boost + quality_boost
        
        return max(100, base_views)
    
    def _fallback_engagement_prediction(self, features: np.ndarray) -> float:
        """Fallback engagement prediction"""
        return 0.05  # 5% baseline
    
    def _fallback_revenue_prediction(self, features: np.ndarray) -> float:
        """Fallback revenue prediction"""
        return 2.50  # $2.50 baseline
    
    def _fallback_viral_prediction(self, features: np.ndarray) -> float:
        """Fallback viral probability prediction"""
        return 0.15  # 15% baseline probability
    
    def _distribute_posting_times(self, schedule: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Distribute posting times to avoid clustering"""
        
        min_interval_hours = 1  # Minimum 1 hour between posts
        
        sorted_schedule = sorted(schedule, key=lambda x: x["optimal_posting_time"])
        
        for i in range(1, len(sorted_schedule)):
            prev_time = sorted_schedule[i-1]["optimal_posting_time"]
            current_time = sorted_schedule[i]["optimal_posting_time"]
            
            time_diff = (current_time - prev_time).total_seconds() / 3600
            
            if time_diff < min_interval_hours:
                # Adjust current posting time
                new_time = prev_time + timedelta(hours=min_interval_hours)
                sorted_schedule[i]["optimal_posting_time"] = new_time
                sorted_schedule[i]["optimization_confidence"] *= 0.9  # Reduce confidence
        
        return sorted_schedule
    
    def _calculate_schedule_improvement(self, optimized_schedule: List[Dict[str, Any]]) -> float:
        """Calculate expected improvement from schedule optimization"""
        
        # Estimate improvement based on optimization confidence
        avg_confidence = sum(item["optimization_confidence"] for item in optimized_schedule) / len(optimized_schedule)
        
        # Conservative improvement estimate
        improvement = avg_confidence * 0.15  # Up to 15% improvement
        
        return round(improvement, 3)
    
    def _estimate_significance_time(self, 
                                  expected_views: int,
                                  expected_conversion_rate: float,
                                  min_detectable_effect: float) -> int:
        """Estimate time to statistical significance for A/B test"""
        
        # Simplified statistical power calculation
        # Real implementation would use proper statistical formulas
        
        sample_size_per_variant = max(100, int(expected_views * 0.5))
        
        # Time estimation based on sample size and traffic
        views_per_hour = max(10, expected_views // 24)
        hours_needed = sample_size_per_variant // views_per_hour
        
        return max(6, min(72, hours_needed))  # 6-72 hours range
    
    def _calculate_optimal_traffic_split(self, performance_pred: ContentPerformancePrediction) -> Dict[str, float]:
        """Calculate optimal traffic split for A/B testing"""
        
        # Higher performing variants get more traffic
        if performance_pred.viral_probability > 0.3:
            return {"control": 0.3, "variant": 0.7}
        elif performance_pred.predicted_views > 5000:
            return {"control": 0.4, "variant": 0.6}
        else:
            return {"control": 0.5, "variant": 0.5}
    
    def _assess_variant_risk(self, performance_pred: ContentPerformancePrediction) -> str:
        """Assess risk level of test variant"""
        
        if performance_pred.confidence_score < 0.6:
            return "high"
        elif performance_pred.viral_probability < 0.1:
            return "medium"
        else:
            return "low"
    
    def _calculate_ab_test_confidence(self, variant_predictions: List[Dict[str, Any]]) -> float:
        """Calculate confidence in A/B test predictions"""
        
        avg_confidence = sum(
            v["predicted_performance"]["confidence_score"] 
            for v in variant_predictions
        ) / len(variant_predictions)
        
        return round(avg_confidence, 3)
    
    def _recommend_early_stopping(self, variant_predictions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Recommend early stopping criteria for A/B test"""
        
        # If there's a clear predicted winner with high confidence
        best_variant = max(variant_predictions, key=lambda x: x["predicted_performance"]["predicted_views"])
        
        if best_variant["predicted_performance"]["confidence_score"] > 0.8:
            return {
                "early_stopping_recommended": True,
                "minimum_hours": 12,
                "confidence_threshold": 0.95
            }
        else:
            return {
                "early_stopping_recommended": False,
                "reason": "Insufficient prediction confidence"
            }
    
    def _prepare_training_data(self, performance_data: List[Dict[str, Any]]) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """Prepare training data for model retraining"""
        
        features_list = []
        labels_dict = {
            "views": [],
            "engagement": [],
            "revenue": [],
            "viral": []
        }
        
        for data_point in performance_data:
            # Extract features
            features = self._extract_content_features(data_point)
            features_list.append(features)
            
            # Extract labels
            labels_dict["views"].append(data_point.get("actual_views", 1000))
            labels_dict["engagement"].append(data_point.get("actual_engagement_rate", 0.05))
            labels_dict["revenue"].append(data_point.get("actual_revenue", 2.50))
            labels_dict["viral"].append(1 if data_point.get("went_viral", False) else 0)
        
        features_array = np.array(features_list)
        labels_arrays = {k: np.array(v) for k, v in labels_dict.items()}
        
        return features_array, labels_arrays
    
    async def _train_individual_model(self, 
                                    model_name: str,
                                    features: np.ndarray,
                                    labels: Dict[str, np.ndarray]):
        """Train individual model with new data"""
        
        # Map model names to label types
        label_mapping = {
            "views_predictor": "views",
            "engagement_predictor": "engagement",
            "revenue_predictor": "revenue",
            "viral_detector": "viral",
            "trend_forecaster": "views"  # Use views as proxy for trend strength
        }
        
        label_key = label_mapping.get(model_name)
        if not label_key or label_key not in labels:
            return
        
        model_labels = labels[label_key]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            features, model_labels, test_size=0.2, random_state=42
        )
        
        # Scale features
        X_train_scaled = self.scalers[model_name].fit_transform(X_train)
        X_test_scaled = self.scalers[model_name].transform(X_test)
        
        # Train model
        self.models[model_name].fit(X_train_scaled, y_train)
        
        # Evaluate model
        if hasattr(self.models[model_name], "predict_proba"):
            # Classification model
            y_pred = self.models[model_name].predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            self.model_metrics[model_name] = PredictionMetrics(
                model_name=model_name,
                mae=0.0,
                accuracy=accuracy,
                confidence_interval=(accuracy - 0.05, accuracy + 0.05),
                last_training=datetime.now(),
                prediction_count=len(X_test)
            )
        else:
            # Regression model
            y_pred = self.models[model_name].predict(X_test_scaled)
            mae = mean_absolute_error(y_test, y_pred)
            
            self.model_metrics[model_name] = PredictionMetrics(
                model_name=model_name,
                mae=mae,
                accuracy=1.0 - mae / (y_test.mean() if y_test.mean() > 0 else 1),
                confidence_interval=(0.8, 0.9),
                last_training=datetime.now(),
                prediction_count=len(X_test)
            )
        
        logger.info(f"ðŸ“ˆ Trained {model_name}: Accuracy/MAE = {self.model_metrics[model_name].accuracy:.3f}")
    
    async def _evaluate_model_performance(self, features: np.ndarray, labels: Dict[str, np.ndarray]):
        """Evaluate overall model performance"""
        
        logger.info("ðŸ“Š Model Performance Summary:")
        for model_name, metrics in self.model_metrics.items():
            logger.info(f"  {model_name}: Accuracy = {metrics.accuracy:.3f}, MAE = {metrics.mae:.3f}")


async def main():
    """Main entry point for predictive analytics"""
    import argparse
    
    parser = argparse.ArgumentParser(description="TenxsomAI Predictive Analytics Engine")
    parser.add_argument("--predict", type=str, help="Predict performance for content (JSON file)")
    parser.add_argument("--train", type=str, help="Train models with data (JSON file)")
    parser.add_argument("--trend", type=str, help="Predict trend evolution for keyword")
    parser.add_argument("--schedule", type=str, help="Optimize posting schedule (JSON file)")
    
    args = parser.parse_args()
    
    # Initialize analytics engine
    analytics = PredictiveAnalyticsEngine()
    
    if args.predict:
        # Load content metadata
        with open(args.predict, 'r') as f:
            content_data = json.load(f)
        
        prediction = await analytics.predict_content_performance(content_data)
        
        print(f"ðŸ“Š Performance Prediction:")
        print(f"Views: {prediction.predicted_views:,}")
        print(f"Engagement: {prediction.predicted_engagement_rate:.1%}")
        print(f"Revenue: ${prediction.predicted_revenue:.2f}")
        print(f"Viral Probability: {prediction.viral_probability:.1%}")
        print(f"Confidence: {prediction.confidence_score:.3f}")
        
        if prediction.optimization_recommendations:
            print(f"\nðŸ’¡ Recommendations:")
            for rec in prediction.optimization_recommendations:
                print(f"  â€¢ {rec}")
    
    if args.trend:
        # Predict trend evolution
        current_metrics = {
            "search_volume": 5000,
            "growth_rate": 25.0,
            "competition_level": 0.3
        }
        
        trend_pred = await analytics.predict_trend_evolution(args.trend, current_metrics)
        
        print(f"ðŸ“ˆ Trend Prediction for '{args.trend}':")
        print(f"Current Momentum: {trend_pred.current_momentum:.3f}")
        print(f"Peak Time: {trend_pred.predicted_peak_time.strftime('%Y-%m-%d %H:%M')}")
        print(f"Peak Value: {trend_pred.predicted_peak_value:,}")
        print(f"Confidence: {trend_pred.confidence:.3f}")
    
    if args.schedule:
        # Optimize posting schedule
        with open(args.schedule, 'r') as f:
            content_queue = json.load(f)
        
        optimized = await analytics.optimize_posting_schedule(content_queue)
        
        print(f"ðŸ“… Optimized Posting Schedule:")
        print(f"Content pieces: {optimized['total_content']}")
        print(f"Expected improvement: {optimized['expected_performance_improvement']:.1%}")
        
        for item in optimized["optimized_schedule"][:5]:  # Show first 5
            content = item["content"]
            time = item["optimal_posting_time"]
            print(f"  {content.get('title', 'Untitled')}: {time.strftime('%m/%d %H:%M')}")


if __name__ == "__main__":
    import time
    asyncio.run(main())